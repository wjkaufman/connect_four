{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect Four AI\n",
    "\n",
    "Wassup! I have no idea what I'm doing, but here goes.\n",
    "\n",
    "So I _finally_ coded up a working Connect Four game in Python, and now comes the fun part. I'm going to follow the style of the tutorial on Medium and start with a simple Q-table implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConnectFour import Game\n",
    "import numpy as np\n",
    "\n",
    "g = Game()\n",
    "g.play(0) # player 1 move\n",
    "g.play(0) # player 2 move\n",
    "g.play(1) # and back to player 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is a sample output of what the computer gets for each move: a \"reward\" (only nonzero when you win/lose), the state of the board, and an indication of whether the game is over or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model config saved to ./saved/model_2_init\n",
      "On episode 0\n",
      "Trained model saved to ./saved/model_2\n",
      "Percent successful episodes: 100.0%\n"
     ]
    }
   ],
   "source": [
    "from ConnectFour import Game\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "g = Game()\n",
    "\n",
    "# constructing the network\n",
    "tf.reset_default_graph()\n",
    "inputs1 = tf.placeholder(shape=[1,84], dtype=tf.float32) # the input is the current state, i.e. the entire board\n",
    "W = tf.Variable(tf.random_uniform([84,7], 0, 0.1)) # the weights for a single layer network are this\n",
    "Qout = tf.matmul(inputs1, W) # predicting Q values using network\n",
    "predict = tf.argmax(Qout, 1) # finding best predicted action from Qout\n",
    "\n",
    "# constructing the loss function\n",
    "nextQ = tf.placeholder(shape=[1,7], dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "updateModel = trainer.minimize(loss)\n",
    "\n",
    "# some magic initializer or something, and a saver\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# hyperparameters\n",
    "y = 0.99 # discount rate for Bellman equation\n",
    "e = 0.1 # chance of random action being taken\n",
    "n_episodes = 50\n",
    "\n",
    "jList = [] # number of steps for each game\n",
    "rList = [] # maximum reward for each game\n",
    "\n",
    "model_num = 2\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # save the current model to file\n",
    "    savePath = saver.save(sess, './saved/model_{}_init'.format(model_num))\n",
    "    print('Initial model config saved to {}'.format(savePath))\n",
    "    \n",
    "    for i in range(n_episodes):\n",
    "        if i%100 is 0:\n",
    "            print('On episode {}'.format(i))\n",
    "        \n",
    "        # reset the game\n",
    "        g.reset()\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        while j < 99:\n",
    "            j += 1\n",
    "            s = g.getState()\n",
    "            a, allQ = sess.run([predict, Qout], feed_dict={inputs1:s.reshape((1,84))})\n",
    "            if np.random.rand(1) < e:\n",
    "                a[0] = np.random.randint(7) # pick a random move\n",
    "            # make the move, get the reward and whatnot\n",
    "            while True: # ensures it makes a move\n",
    "                try:\n",
    "                    s1, r, d = g.play(a[0])\n",
    "                    break\n",
    "                except:\n",
    "                    # first check if game is still playable\n",
    "                    if g.isPlayable():\n",
    "                        a[0] = np.random.randint(7)\n",
    "                    else:\n",
    "                        s1 = s\n",
    "                        r = 0\n",
    "                        d = True\n",
    "                        break\n",
    "            Q1 = sess.run(Qout, {inputs1:s1.reshape((1,84))})\n",
    "            maxQ1 = np.max(Q1)\n",
    "            targetQ = allQ\n",
    "            # not sure why it's 0, a[0] here... Wait, I think it's because\n",
    "            # targetQ is 1x7 vector, and only want to update the Q value for\n",
    "            # the given state/action pair\n",
    "            targetQ[0, a[0]] = r + y*maxQ1\n",
    "            # train model based on new value\n",
    "            _, W1 = sess.run([updateModel, W], {inputs1: s.reshape((1,84)), nextQ: targetQ})\n",
    "            rAll += r\n",
    "            \n",
    "            if d:\n",
    "                e = 1. / ((i/50.) + 10)\n",
    "                break\n",
    "        \n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "    \n",
    "    # save the current model and vars to file\n",
    "    savePath = saver.save(sess, './saved/model_{}'.format(model_num))\n",
    "    print('Trained model saved to {}'.format(savePath))\n",
    "\n",
    "print('Percent successful episodes: {}%'.format(sum(rList)/n_episodes * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(rList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(jList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So... What did I just do?\n",
    "\n",
    "I'm not really sure, but the computer did run 2000 episodes of Connect Four and (possibly) trained the Q-network to play. I'm not really sure how to test it, and I've got to run to practice now, but I'll return to this when I get back.\n",
    "\n",
    "Ok! So I messed around with some stuff above to save/reload models, and now below I'm going to try to implement a me-vs-machine game thingy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saved/model_2\n",
      "-\t-\t-\t-\t-\t-\t-\t\n",
      "-\t-\t-\t-\t-\t-\t-\t\n",
      "-\t-\t-\t-\t-\t-\t-\t\n",
      "-\t-\t-\t-\t-\t-\t-\t\n",
      "-\t-\t-\t-\t-\t-\t-\t\n",
      "-\t-\t-\t-\t-\t-\t-\t\n",
      "---\t---\t---\t---\t---\t---\t---\t\n",
      "1\t2\t3\t4\t5\t6\t7\n",
      "current player is 1\n",
      "Human, enter move: 1\n",
      "Computer is thinking...\n",
      "Computer is making move...\n",
      "Computer made move in column 1.\n",
      "-\t-\t-\t-\t-\t-\t-\t\n",
      "-\t-\t-\t-\t-\t-\t-\t\n",
      "-\t-\t-\t-\t-\t-\t-\t\n",
      "-\t-\t-\t-\t-\t-\t-\t\n",
      "2\t-\t-\t-\t-\t-\t-\t\n",
      "1\t-\t-\t-\t-\t-\t-\t\n",
      "---\t---\t---\t---\t---\t---\t---\t\n",
      "1\t2\t3\t4\t5\t6\t7\n",
      "current player is 1\n",
      "Human, enter move: 2\n",
      "Computer is thinking...\n",
      "Computer is making move...\n",
      "Computer made move in column 6.\n",
      "-\t-\t-\t-\t-\t-\t-\t\n",
      "-\t-\t-\t-\t-\t-\t-\t\n",
      "-\t-\t-\t-\t-\t-\t-\t\n",
      "-\t-\t-\t-\t-\t-\t-\t\n",
      "2\t-\t-\t-\t-\t-\t-\t\n",
      "1\t1\t-\t-\t-\t2\t-\t\n",
      "---\t---\t---\t---\t---\t---\t---\t\n",
      "1\t2\t3\t4\t5\t6\t7\n",
      "current player is 1\n",
      "Human, enter move: 2\n",
      "Computer is thinking...\n",
      "Computer is making move...\n",
      "Computer made move in column 6.\n",
      "-\t-\t-\t-\t-\t-\t-\t\n",
      "-\t-\t-\t-\t-\t-\t-\t\n",
      "-\t-\t-\t-\t-\t-\t-\t\n",
      "-\t-\t-\t-\t-\t-\t-\t\n",
      "2\t1\t-\t-\t-\t2\t-\t\n",
      "1\t1\t-\t-\t-\t2\t-\t\n",
      "---\t---\t---\t---\t---\t---\t---\t\n",
      "1\t2\t3\t4\t5\t6\t7\n",
      "current player is 1\n",
      "Human, enter move: 2\n",
      "Computer is thinking...\n",
      "Computer is making move...\n",
      "Computer made move in column 3.\n",
      "-\t-\t-\t-\t-\t-\t-\t\n",
      "-\t-\t-\t-\t-\t-\t-\t\n",
      "-\t-\t-\t-\t-\t-\t-\t\n",
      "-\t1\t-\t-\t-\t-\t-\t\n",
      "2\t1\t-\t-\t-\t2\t-\t\n",
      "1\t1\t2\t-\t-\t2\t-\t\n",
      "---\t---\t---\t---\t---\t---\t---\t\n",
      "1\t2\t3\t4\t5\t6\t7\n",
      "current player is 1\n",
      "Human, enter move: 2\n",
      "Winner was 1\n"
     ]
    }
   ],
   "source": [
    "# create a game\n",
    "g = Game()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    new_saver = tf.train.import_meta_graph('./saved/model_1.meta')\n",
    "    new_saver.restore(sess, tf.train.latest_checkpoint('./saved'))\n",
    "    \n",
    "    d = False\n",
    "    while not d:\n",
    "        print(g)\n",
    "        # get human input\n",
    "        while True:\n",
    "            try:\n",
    "                play = int(input(\"Human, enter move: \")) - 1\n",
    "                _, r, d = g.play(play)\n",
    "                break\n",
    "            except:\n",
    "                if g.isPlayable():\n",
    "                    print(\"Error, try again\")\n",
    "                else:\n",
    "                    d = True\n",
    "                    break\n",
    "        if d:\n",
    "            break\n",
    "        # then get computer input, first predict best move from Q-network\n",
    "        print(\"Computer is thinking...\")\n",
    "        s = g.getState()\n",
    "        a, allQ = sess.run([predict, Qout], {inputs1: s.reshape((1,84))})\n",
    "        # then make the move\n",
    "        print(\"Computer is making move...\")\n",
    "        while True:\n",
    "            try:\n",
    "                _, r, d = g.play(a[0])\n",
    "                break\n",
    "            except:\n",
    "                if g.isPlayable():\n",
    "                    a[0] = np.random.randint(7)\n",
    "                else:\n",
    "                    d = True\n",
    "                    break\n",
    "        print(\"Computer made move in column {}.\".format(a[0] + 1))\n",
    "    print(\"Winner was {}\".format(g.getWinner()))\n",
    "    print(g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
